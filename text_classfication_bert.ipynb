{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import torch\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from database_commands.create_tables import app, mysql, Patient, Doctor, PatientDoctor, ClinicalInfo, Treatment\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['patient_id','name','age', 'sex', 'id.1', 'patient_id.1', 'doctor_id', 'id.2', 'name.1', 'specialty', 'contact_info', 'id.3', 'patient_id.2', 'doctor_id.1', 'histological_subtype','second_tumor_size', 'third_tumor_size', 'method', 'ki_67', 'clinical_stage'], inplace=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['node_status', 'tnm_tumor', 'tumor_grade']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  tumor_size  tumor_grade  node_status  metastasis tnm_tumor  er_status  \\\n",
      "1    2         3.0          2.0          1.0           0         3          1   \n",
      "7   12         4.7          2.0          0.0           0         2          1   \n",
      "16  22         3.4          3.0          1.0           0         3          0   \n",
      "28  29         5.0          1.0          1.0           1         3          0   \n",
      "\n",
      "    pr_status  her2_status  ki_67_categorised  \\\n",
      "1           1            0                  3   \n",
      "7           0            0                  3   \n",
      "16          0            0                  3   \n",
      "28          0            0                  3   \n",
      "\n",
      "                                 final_treatment_plan  \n",
      "1   Mastectomy, Axillary Surgery, Adjuvant Chemoth...  \n",
      "7   Mastectomy, Axillary Surgery, Adjuvant Chemoth...  \n",
      "16  Neoadjuvant Chemotherapy, Mastectomy, Axillary...  \n",
      "28               Neoadjuvant Chemotherapy, Mastectomy  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "test_df = df[df[\"id\"].isin([2, 12, 22, 29])]\n",
    "val_df = df[df[\"id\"].isin([6, 10, 16, 21, 13, 45, 25, 28])]\n",
    "train_df = df[~df[\"id\"].isin([2, 12, 22, 29, 6, 10, 16, 21, 13, 45, 25, 28])].copy()\n",
    "\n",
    "print(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned labels: ['Adjuvant Chemotherapy' 'Axillary Surgery' 'Breast Conserving Surgery'\n",
      " 'Herceptin' 'Hormonal Therapy' 'Mastectomy' 'Neoadjuvant Chemotherapy'\n",
      " 'Radiotherapy']\n",
      "Class counts: [15 20  8  2 13 15 12 20]\n",
      "Class weights: {0: np.float64(0.875), 1: np.float64(0.65625), 2: np.float64(1.640625), 3: np.float64(6.5625), 4: np.float64(1.0096153846153846), 5: np.float64(0.875), 6: np.float64(1.09375), 7: np.float64(0.65625)}\n",
      "Class Weights Tensor: tensor([0.8750, 0.6562, 1.6406, 6.5625, 1.0096, 0.8750, 1.0938, 0.6562])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "labels = [list(map(str.strip, plan.split(\",\"))) for plan in train_df[\"final_treatment_plan\"]]\n",
    "mlb.fit(labels)\n",
    "num_classes = len(mlb.classes_)\n",
    "\n",
    "labels_bin = mlb.transform(labels)\n",
    "print(\"Learned labels:\", mlb.classes_)\n",
    "class_counts = np.sum(labels_bin, axis=0)\n",
    "total_labels = np.sum(class_counts)\n",
    "class_weights = {i: total_labels / (len(class_counts) * count) if count > 0 else 1.0\n",
    "                 for i, count in enumerate(class_counts)}\n",
    "\n",
    "class_weights_list = [class_weights.get(i, 1.0) for i in range(len(mlb.classes_))]\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights_list, dtype=torch.float32)\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(f\"Class Weights Tensor: {class_weights_tensor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 24\n",
      "Oversampled dataset size: 79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "train_df[\"final_treatment_plan\"] = train_df[\"final_treatment_plan\"].str.split(\", \")\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels_bin = mlb.fit_transform(train_df[\"final_treatment_plan\"])\n",
    "df_labels = pd.DataFrame(labels_bin, columns=mlb.classes_)\n",
    "label_counts = df_labels.sum(axis=0)\n",
    "max_count = label_counts.max()\n",
    "\n",
    "df_oversampled = train_df.copy()\n",
    "for label in label_counts.index:\n",
    "    if label_counts[label] < max_count:\n",
    "        subset = train_df[train_df[\"final_treatment_plan\"].apply(lambda x: label in x)]\n",
    "        num_samples_needed = max_count - label_counts[label]\n",
    "        resampled_subset = resample(subset, replace=True, n_samples=num_samples_needed, random_state=42)\n",
    "        df_oversampled = pd.concat([df_oversampled, resampled_subset])\n",
    "\n",
    "df_oversampled = df_oversampled.reset_index(drop=True)\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_oversampled),\n",
    "    \"validation\": Dataset.from_pandas(val_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "})\n",
    "print(f\"Original dataset size: {len(train_df)}\")\n",
    "print(f\"Oversampled dataset size: {len(df_oversampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tumor_size', 'tumor_grade', 'node_status', 'metastasis', 'tnm_tumor', 'er_status', 'pr_status', 'her2_status', 'ki_67_categorised', 'final_treatment_plan', '__index_level_0__'],\n",
      "        num_rows: 55\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tumor_size', 'tumor_grade', 'node_status', 'metastasis', 'tnm_tumor', 'er_status', 'pr_status', 'her2_status', 'ki_67_categorised', 'final_treatment_plan', '__index_level_0__'],\n",
      "        num_rows: 14\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tumor_size', 'tumor_grade', 'node_status', 'metastasis', 'tnm_tumor', 'er_status', 'pr_status', 'her2_status', 'ki_67_categorised', 'final_treatment_plan', '__index_level_0__'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "mlb_split = MultiLabelBinarizer()\n",
    "y = mlb_split.fit_transform(df_oversampled[\"final_treatment_plan\"])\n",
    "X = np.array(df_oversampled.index).reshape(-1, 1)\n",
    "\n",
    "X_train, y_train, X_temp, y_temp = iterative_train_test_split(X, y, test_size=0.3)\n",
    "X_val, y_val, X_test, y_test = iterative_train_test_split(X_temp, y_temp, test_size=0.4)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_oversampled.iloc[X_train.flatten()]),\n",
    "    \"validation\": Dataset.from_pandas(df_oversampled.iloc[X_val.flatten()]),\n",
    "    \"test\": Dataset.from_pandas(df_oversampled.iloc[X_test.flatten()])\n",
    "})\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 1781.34 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 916.79 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 379.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def create_text_representation(example):\n",
    "    return {\n",
    "        \"text\": f\"Tumor size: {example['tumor_size']}, Grade: {example['tumor_grade']}, \"\n",
    "                f\"Node status: {example['node_status']}, Metastasis: {example['metastasis']}, \"\n",
    "                f\"TNM: {example['tnm_tumor']}, ER: {example['er_status']}, \"\n",
    "                f\"PR: {example['pr_status']}, HER2: {example['her2_status']}, \"\n",
    "                f\"Ki-67: {example['ki_67_categorised']}\"\n",
    "    }\n",
    "\n",
    "ds = ds.map(create_text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(mlb, 'label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 2185.92 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 799.41 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 436.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def encode_labels(example):\n",
    "    labels = example[\"final_treatment_plan\"]\n",
    "    if isinstance(labels, str):\n",
    "        labels = labels.split(\", \") \n",
    "    binarized_labels = mlb.transform([labels])[0]\n",
    "    return {\"labels\": list(map(float, binarized_labels))}\n",
    "\n",
    "labelled_ds = ds.map(encode_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 958.62 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 567.29 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 431.77 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'labels': tensor([1., 1., 0., 0., 1., 1., 0., 1.]),\n",
       "  'input_ids': tensor([    2,  2798,  3081,    30,    24,    18,    27,    16,  4927,    30,\n",
       "             23,    18,    20,    16,  6267,  3642,    30,    21,    18,    20,\n",
       "             16,  5967,    30,    20,    16, 19400,    30,    23,    16,  2668,\n",
       "             30,    21,    16,  2033,    30,    21,    16,  9162,    30,    21,\n",
       "             16,  8191,    17,  4838,    30,    23,     3,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " {'labels': tensor([1., 1., 0., 0., 0., 1., 0., 1.]),\n",
       "  'input_ids': tensor([    2,  2798,  3081,    30,    23,    18,    20,    16,  4927,    30,\n",
       "             23,    18,    20,    16,  6267,  3642,    30,    21,    18,    20,\n",
       "             16,  5967,    30,    20,    16, 19400,    30,  6887,    16,  2668,\n",
       "             30,    20,    16,  2033,    30,    20,    16,  9162,    30,    20,\n",
       "             16,  8191,    17,  4838,    30,    23,     3,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " {'labels': tensor([0., 1., 0., 0., 0., 1., 1., 1.]),\n",
       "  'input_ids': tensor([    2,  2798,  3081,    30,    24,    18,    20,    16,  4927,    30,\n",
       "             23,    18,    20,    16,  6267,  3642,    30,    21,    18,    20,\n",
       "             16,  5967,    30,    20,    16, 19400,    30,  6887,    16,  2668,\n",
       "             30,    20,    16,  2033,    30,    20,    16,  9162,    30,    20,\n",
       "             16,  8191,    17,  4838,    30,    23,     3,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0])})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_ds = labelled_ds.map(tokenize_function, batched=True)\n",
    "tokenized_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "tokenized_ds['validation'][0],tokenized_ds['train'][20],tokenized_ds['train'][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", num_labels=len(mlb.classes_), problem_type=\"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs[\"labels\"] \n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(weight=class_weights_tensor) \n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\OneDrive\\Comp Sci Uni\\Project\\diagnosis_website\\venv\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1.288375733581335e-05,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.011386646772668109,\n",
    "    warmup_ratio=0.01662716373178627,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 17:08, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.040119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.999417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.987248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.977278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.976778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.972021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=1.0017630259195964, metrics={'train_runtime': 1052.5114, 'train_samples_per_second': 0.45, 'train_steps_per_second': 0.057, 'total_flos': 124721358815232.0, 'train_loss': 1.0017630259195964, 'epoch': 6.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds (from validation set): [0.67067975 0.6812135  0.29535976 0.35007584 0.49219078 0.57367903\n",
      " 0.60886598 0.72942328]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Adjuvant Chemotherapy, Predictions Made: 4, Actual Frequency: 2.0\n",
      "Label: Axillary Surgery, Predictions Made: 4, Actual Frequency: 3.0\n",
      "Label: Breast Conserving Surgery, Predictions Made: 3, Actual Frequency: 0.0\n",
      "Label: Herceptin, Predictions Made: 0, Actual Frequency: 0.0\n",
      "Label: Hormonal Therapy, Predictions Made: 2, Actual Frequency: 2.0\n",
      "Label: Mastectomy, Predictions Made: 4, Actual Frequency: 4.0\n",
      "Label: Neoadjuvant Chemotherapy, Predictions Made: 3, Actual Frequency: 2.0\n",
      "Label: Radiotherapy, Predictions Made: 4, Actual Frequency: 3.0\n",
      "Sample 1:\n",
      "  Prediction: ('Adjuvant Chemotherapy', 'Axillary Surgery', 'Breast Conserving Surgery', 'Hormonal Therapy', 'Mastectomy', 'Neoadjuvant Chemotherapy', 'Radiotherapy')\n",
      "  Actual: ('Adjuvant Chemotherapy', 'Axillary Surgery', 'Hormonal Therapy', 'Mastectomy', 'Radiotherapy')\n",
      "  Correct Predictions: 5/5 (100.00%)\n",
      "--------------------------------------------------\n",
      "Sample 2:\n",
      "  Prediction: ('Adjuvant Chemotherapy', 'Axillary Surgery', 'Breast Conserving Surgery', 'Hormonal Therapy', 'Mastectomy', 'Radiotherapy')\n",
      "  Actual: ('Adjuvant Chemotherapy', 'Axillary Surgery', 'Hormonal Therapy', 'Mastectomy', 'Radiotherapy')\n",
      "  Correct Predictions: 5/5 (100.00%)\n",
      "--------------------------------------------------\n",
      "Sample 3:\n",
      "  Prediction: ('Adjuvant Chemotherapy', 'Axillary Surgery', 'Mastectomy', 'Neoadjuvant Chemotherapy', 'Radiotherapy')\n",
      "  Actual: ('Axillary Surgery', 'Mastectomy', 'Neoadjuvant Chemotherapy', 'Radiotherapy')\n",
      "  Correct Predictions: 4/4 (100.00%)\n",
      "--------------------------------------------------\n",
      "Sample 4:\n",
      "  Prediction: ('Adjuvant Chemotherapy', 'Axillary Surgery', 'Breast Conserving Surgery', 'Mastectomy', 'Neoadjuvant Chemotherapy', 'Radiotherapy')\n",
      "  Actual: ('Mastectomy', 'Neoadjuvant Chemotherapy')\n",
      "  Correct Predictions: 2/2 (100.00%)\n",
      "--------------------------------------------------\n",
      "Macro F1 Score: 0.6476\n",
      "Micro F1 Score: 0.8000\n",
      "Micro Precision: 0.6667\n",
      "Micro Recall: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\OneDrive\\Comp Sci Uni\\Project\\diagnosis_website\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "from transformers import Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score,  precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "trainer = Trainer(model=model) \n",
    "mlb = joblib.load(\"label_encoder.pkl\")\n",
    "validation_logits = trainer.predict(tokenized_ds[\"validation\"]).predictions\n",
    "validation_probs = expit(validation_logits) \n",
    "\n",
    "labels = np.array(tokenized_ds[\"validation\"][\"labels\"])\n",
    "optimal_thresholds = np.zeros(validation_probs.shape[1])\n",
    "\n",
    "for i in range(validation_probs.shape[1]):\n",
    "    precision, recall, thresholds = precision_recall_curve(labels[:, i], validation_probs[:, i])\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    best_label_threshold = thresholds[np.argmax(f1_scores)] \n",
    "    optimal_thresholds[i] = best_label_threshold\n",
    "joblib.dump(optimal_thresholds, \"optimal_thresholds.pkl\")\n",
    "print(f\"Thresholds (from validation set): {optimal_thresholds}\")\n",
    "\n",
    "labels = np.array(tokenized_ds[\"test\"][\"labels\"])\n",
    "test_logits = trainer.predict(tokenized_ds[\"test\"]).predictions\n",
    "test_probs = expit(test_logits)\n",
    "predictions = (test_probs > optimal_thresholds).astype(int)\n",
    "\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    label_pred_count = np.sum(test_probs[:, i] > optimal_thresholds[i])\n",
    "    label_true_count = np.sum(labels[:, i]) \n",
    "    print(f\"Label: {label}, Predictions Made: {label_pred_count}, Actual Frequency: {label_true_count}\")\n",
    "\n",
    "labels = np.array(tokenized_ds[\"test\"][\"labels\"])\n",
    "predictions_text = mlb.inverse_transform(predictions)\n",
    "labels_text = mlb.inverse_transform(labels)\n",
    "\n",
    "correct_counts = []\n",
    "total_counts = []\n",
    "per_question_f1_scores = []\n",
    "\n",
    "for i, (pred, true) in enumerate(zip(predictions_text, labels_text)):\n",
    "    pred_set = set(pred)\n",
    "    true_set = set(true)\n",
    "    \n",
    "    correct = len(pred_set & true_set) \n",
    "    total = len(true_set)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    correct_counts.append(correct)\n",
    "    total_counts.append(total)\n",
    "\n",
    "    sample_f1 = f1_score(labels[i], predictions[i], average=\"micro\") \n",
    "    per_question_f1_scores.append(sample_f1)\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Prediction: {pred}\")\n",
    "    print(f\"  Actual: {true}\")\n",
    "    print(f\"  Correct Predictions: {correct}/{total} ({accuracy:.2%})\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "macro_f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "micro_f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "micro_precision = precision_score(labels, predictions, average=\"micro\")\n",
    "micro_recall = recall_score(labels, predictions, average=\"micro\")\n",
    "mean_f1_per_question = np.mean(per_question_f1_scores)\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "print(f\"Micro Precision: {micro_precision:.4f}\")\n",
    "print(f\"Micro Recall: {micro_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_thresholds = joblib.load('optimal_thresholds.pkl')\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "inputs = tokenizer(\"\", truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "probabilities = expit(logits.detach().cpu().numpy())\n",
    "predictions = (probabilities > optimal_thresholds).astype(int)  \n",
    "predicted_labels = label_encoder.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "metrics = {}\n",
    "metrics['Text-Classification'] = {\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"micro_precision\": micro_precision,\n",
    "        \"micro_recall\": micro_recall,\n",
    "        \"mean_f1_per_question\": mean_f1_per_question,\n",
    "        \"f1_per_question\": per_question_f1_scores\n",
    "    }\n",
    "with open(\"model_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/bert_model/text_classification\\\\tokenizer_config.json',\n",
       " 'models/bert_model/text_classification\\\\special_tokens_map.json',\n",
       " 'models/bert_model/text_classification\\\\vocab.txt',\n",
       " 'models/bert_model/text_classification\\\\added_tokens.json',\n",
       " 'models/bert_model/text_classification\\\\tokenizer.json')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"models/bert_model/text_classification\")\n",
    "tokenizer.save_pretrained(\"models/bert_model/text_classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
